{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50b9f4-4430-4bd6-bd01-b6cde44f6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-Model Ensemble Generator for Jordan Climate Data\n",
    "# Creates ensemble from all 6 available climate models uniformly across all basins\n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_basin_mask(basins_gdf, lat_points, lon_points, jordan_gdf):\n",
    "    \"\"\"Create a mask array with expanded coverage to ensure all stations get data\"\"\"\n",
    "    # Ensure consistent CRS\n",
    "    basins_gdf = basins_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "    jordan_gdf = jordan_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "\n",
    "    mask = np.full((len(lat_points), len(lon_points)), None, dtype=object)\n",
    "\n",
    "    # Create mesh grid of points\n",
    "    lon_mesh, lat_mesh = np.meshgrid(lon_points, lat_points)\n",
    "\n",
    "    # Get Jordan boundary with a buffer for edge cases\n",
    "    jordan_boundary = jordan_gdf.geometry.union_all()\n",
    "\n",
    "    # Create an expanded boundary - add ~0.2 degrees (~22km) buffer around Jordan\n",
    "    jordan_boundary_buffered = jordan_boundary.buffer(0.2)\n",
    "\n",
    "    # Filter out Syria basins\n",
    "    basins_filtered = basins_gdf[~basins_gdf['BASIN_NAME'].str.contains('SYRIA', case=False, na=False)].copy()\n",
    "    basins_filtered['geometry'] = basins_filtered.geometry.intersection(jordan_boundary)\n",
    "    basins_filtered = basins_filtered[~basins_filtered.geometry.is_empty]\n",
    "\n",
    "    # Get grid bounds to identify boundary regions\n",
    "    min_lat, max_lat = min(lat_points), max(lat_points)\n",
    "    min_lon, max_lon = min(lon_points), max(lon_points)\n",
    "\n",
    "    # Calculate grid spacing\n",
    "    lat_spacing = lat_points[1] - lat_points[0] if len(lat_points) > 1 else 0.1\n",
    "    lon_spacing = lon_points[1] - lon_points[0] if len(lon_points) > 1 else 0.1\n",
    "\n",
    "    # Expand processing region by 2 grid cells as requested\n",
    "    processing_min_lat = min_lat - 2 * lat_spacing\n",
    "    processing_max_lat = max_lat + 2 * lat_spacing\n",
    "    processing_min_lon = min_lon - 2 * lon_spacing\n",
    "    processing_max_lon = max_lon + 2 * lon_spacing\n",
    "\n",
    "    assigned_count = 0\n",
    "    boundary_assignments = 0\n",
    "    nearest_assignments = 0\n",
    "\n",
    "    print(f\"Processing expanded grid region:\")\n",
    "    print(f\"  Original grid: {min_lat:.3f} to {max_lat:.3f} lat, {min_lon:.3f} to {max_lon:.3f} lon\")\n",
    "    print(\n",
    "        f\"  Expanded region: {processing_min_lat:.3f} to {processing_max_lat:.3f} lat, {processing_min_lon:.3f} to {processing_max_lon:.3f} lon\")\n",
    "    print(f\"  Grid spacing: {lat_spacing:.3f} lat, {lon_spacing:.3f} lon\")\n",
    "\n",
    "    # Assign each point to a basin with multiple fallback strategies\n",
    "    for i in range(len(lat_points)):\n",
    "        for j in range(len(lon_points)):\n",
    "            point = Point(lon_mesh[i, j], lat_mesh[i, j])\n",
    "            current_lat = lat_points[i]\n",
    "            current_lon = lon_points[j]\n",
    "\n",
    "            # Strategy 1: Direct basin containment\n",
    "            basin_found = False\n",
    "            for idx, basin in basins_filtered.iterrows():\n",
    "                if basin.geometry.contains(point):\n",
    "                    mask[i, j] = basin['BASIN_NAME']\n",
    "                    basin_found = True\n",
    "                    assigned_count += 1\n",
    "                    break\n",
    "\n",
    "            if basin_found:\n",
    "                continue\n",
    "\n",
    "            # Strategy 2: Point within Jordan boundary (original or buffered)\n",
    "            if jordan_boundary.contains(point) or jordan_boundary_buffered.contains(point):\n",
    "                # Find nearest basin for points in Jordan\n",
    "                min_distance = float('inf')\n",
    "                nearest_basin = None\n",
    "\n",
    "                for idx, basin in basins_filtered.iterrows():\n",
    "                    distance = basin.geometry.distance(point)\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        nearest_basin = basin['BASIN_NAME']\n",
    "\n",
    "                if nearest_basin and min_distance < 0.5:  # Within ~55km\n",
    "                    mask[i, j] = nearest_basin\n",
    "                    assigned_count += 1\n",
    "                    boundary_assignments += 1\n",
    "                    if min_distance > 0.1:  # Log if it's a significant distance assignment\n",
    "                        print(\n",
    "                            f\"    Boundary assignment: Grid ({current_lat:.3f}, {current_lon:.3f}) → {nearest_basin} ({min_distance * 111:.1f} km)\")\n",
    "                continue\n",
    "\n",
    "            # Strategy 3: Extended region coverage (for stations near borders)\n",
    "            # Check if point is in our expanded processing region\n",
    "            if (processing_min_lat <= current_lat <= processing_max_lat and\n",
    "                    processing_min_lon <= current_lon <= processing_max_lon):\n",
    "\n",
    "                # Find nearest basin regardless of boundaries for border regions\n",
    "                min_distance = float('inf')\n",
    "                nearest_basin = None\n",
    "\n",
    "                for idx, basin in basins_filtered.iterrows():\n",
    "                    distance = basin.geometry.distance(point)\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        nearest_basin = basin['BASIN_NAME']\n",
    "\n",
    "                if nearest_basin and min_distance < 1.0:  # Within ~111km for border regions\n",
    "                    mask[i, j] = nearest_basin\n",
    "                    assigned_count += 1\n",
    "                    nearest_assignments += 1\n",
    "                    print(\n",
    "                        f\"    Extended assignment: Grid ({current_lat:.3f}, {current_lon:.3f}) → {nearest_basin} ({min_distance * 111:.1f} km)\")\n",
    "\n",
    "    print(f\"Basin mask creation summary:\")\n",
    "    print(f\"  Total assignments: {assigned_count}\")\n",
    "    print(f\"  Direct basin containment: {assigned_count - boundary_assignments - nearest_assignments}\")\n",
    "    print(f\"  Boundary assignments: {boundary_assignments}\")\n",
    "    print(f\"  Extended region assignments: {nearest_assignments}\")\n",
    "    print(\n",
    "        f\"  Coverage: {assigned_count}/{len(lat_points) * len(lon_points)} grid points ({assigned_count / (len(lat_points) * len(lon_points)) * 100:.1f}%)\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_6_model_ensemble_for_period(nc_dir, basin_mask, model_names,\n",
    "                                       start_date, end_date, output_path):\n",
    "    \"\"\"Create ensemble dataset using all 6 models for a specific time period\"\"\"\n",
    "    # Open one NC file to get coordinate reference\n",
    "    sample_ds = xr.open_dataset(Path(nc_dir) / f\"{model_names[0]}.nc\")\n",
    "\n",
    "    # Select time period\n",
    "    time_slice = slice(start_date, end_date)\n",
    "\n",
    "    # Initialize output dataset\n",
    "    ds_out = xr.Dataset(\n",
    "        coords={\n",
    "            'time': sample_ds.time.sel(time=time_slice),\n",
    "            'lat': sample_ds.lat,\n",
    "            'lon': sample_ds.lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create empty array for ensemble data\n",
    "    ensemble_data = np.zeros((len(ds_out.time), len(ds_out.lat), len(ds_out.lon)))\n",
    "    model_count = np.zeros((len(ds_out.lat), len(ds_out.lon)), dtype=int)\n",
    "\n",
    "    # Process each grid point\n",
    "    print(f\"\\nProcessing period {start_date} to {end_date}\")\n",
    "    total_points = len(ds_out.lat) * len(ds_out.lon)\n",
    "    processed_points = 0\n",
    "\n",
    "    for i in range(len(ds_out.lat)):\n",
    "        for j in range(len(ds_out.lon)):\n",
    "            basin = basin_mask[i, j]\n",
    "            if basin is not None:  # Point is within a Jordan basin\n",
    "                point_data = []\n",
    "                successful_models = []\n",
    "\n",
    "                # Load data from all 6 models for this grid point\n",
    "                for model in model_names:\n",
    "                    try:\n",
    "                        ds = xr.open_dataset(Path(nc_dir) / f\"{model}.nc\")\n",
    "                        model_data = ds.prAdjust.sel(time=time_slice).isel(lat=i, lon=j)\n",
    "                        point_data.append(model_data.values)\n",
    "                        successful_models.append(model)\n",
    "                        ds.close()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not load {model} for point ({i},{j}): {e}\")\n",
    "                        continue\n",
    "\n",
    "                if point_data:\n",
    "                    # Calculate ensemble mean of all available models\n",
    "                    ensemble_data[:, i, j] = np.mean(point_data, axis=0)\n",
    "                    model_count[i, j] = len(point_data)\n",
    "\n",
    "            processed_points += 1\n",
    "            if processed_points % 1000 == 0:\n",
    "                print(\n",
    "                    f\"Processed {processed_points}/{total_points} points ({processed_points / total_points * 100:.1f}%)\")\n",
    "\n",
    "    # Add variables to output dataset\n",
    "    ds_out['prAdjust'] = (('time', 'lat', 'lon'), ensemble_data)\n",
    "    ds_out['model_count'] = (('lat', 'lon'), model_count)\n",
    "\n",
    "    # Add metadata\n",
    "    ds_out.prAdjust.attrs = sample_ds.prAdjust.attrs\n",
    "    ds_out.prAdjust.attrs['description'] = 'Ensemble average of precipitation from all 6 climate models'\n",
    "    ds_out.prAdjust.attrs['models_used'] = ', '.join(model_names)\n",
    "    ds_out.model_count.attrs['description'] = 'Number of models used in ensemble average at each point'\n",
    "    ds_out.attrs['description'] = f'Ensemble average of 6 climate models ({start_date} to {end_date})'\n",
    "    ds_out.attrs['models'] = ', '.join(model_names)\n",
    "    ds_out.attrs['period'] = f'{start_date} to {end_date}'\n",
    "    ds_out.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "    ds_out.attrs['ensemble_type'] = '6-model uniform ensemble'\n",
    "\n",
    "    # Save to file\n",
    "    print(f\"Saving to {output_path}\")\n",
    "    ds_out.to_netcdf(output_path)\n",
    "    sample_ds.close()\n",
    "    return ds_out\n",
    "\n",
    "\n",
    "def create_ensemble_summary_report(output_dir, model_names):\n",
    "    \"\"\"Create a summary report of the ensemble creation process\"\"\"\n",
    "    report_content = f\"\"\"\n",
    "# 6-Model Ensemble Summary Report\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Models Used:\n",
    "{chr(10).join(f\"{i + 1}. {model}\" for i, model in enumerate(model_names))}\n",
    "\n",
    "## Ensemble Method:\n",
    "- Uniform application of all 6 models across all grid points\n",
    "- Simple arithmetic mean of all available models at each point\n",
    "- Geographic coverage: Jordan basins only (Syria basins excluded)\n",
    "\n",
    "## Time Periods Processed:\n",
    "1. 1961_1994: Historical baseline period\n",
    "2. 1995_2014: Recent historical period  \n",
    "3. 2015_2020: Recent period\n",
    "4. 2021_2040: Near-future projections\n",
    "5. 2041_2060: Mid-future projections\n",
    "6. 2061_2070: Far-future projections\n",
    "\n",
    "## Output Files:\n",
    "- ensemble_precipitation_6models_1961_1994.nc\n",
    "- ensemble_precipitation_6models_1995_2014.nc\n",
    "- ensemble_precipitation_6models_2015_2020.nc\n",
    "- ensemble_precipitation_6models_2021_2040.nc\n",
    "- ensemble_precipitation_6models_2041_2060.nc\n",
    "- ensemble_precipitation_6models_2061_2070.nc\n",
    "\n",
    "## Technical Details:\n",
    "- Variable: prAdjust (adjusted precipitation)\n",
    "- Grid: 85x75 points\n",
    "- CRS: EPSG:4326 (WGS84)\n",
    "- Units: mm/day\n",
    "- Missing values handled: Yes\n",
    "\"\"\"\n",
    "\n",
    "    with open(Path(output_dir) / 'ensemble_summary_report.txt', 'w') as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for creating 6-model ensemble datasets\"\"\"\n",
    "\n",
    "    # Define paths\n",
    "    nc_dir = r\"D:\\RICAAR\\riccar-data_jordan-ssp2-4-5-daily-data_2024-07-29_0915\\Merge\\Precipitation\"\n",
    "    basin_shapefile = r\"D:\\RICAAR\\surfacebasin\\surface_basin.shp\"\n",
    "    gov_shapefile = r\"D:\\RICAAR\\Governorates\\JordanwithGovernorates.shp\"\n",
    "    output_dir = r\"D:\\RICAAR\\Pr.New.Stations.Selection\\ensemble.models.6.models\"\n",
    "\n",
    "    # Define all 6 model names\n",
    "    model_names = [\n",
    "        'CMCC-CM2-SR5',\n",
    "        'CNRM-ESM2-1',\n",
    "        'EC-Earth3-Veg',\n",
    "        'IPSL-CM6A-LR',\n",
    "        'MPI-ESM1-2-LR',\n",
    "        'NorESM2-MM'\n",
    "    ]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(\"=== 6-Model Ensemble Generator ===\")\n",
    "        print(f\"Using models: {', '.join(model_names)}\")\n",
    "        print(f\"Input directory: {nc_dir}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        # Verify all model files exist\n",
    "        print(\"\\nVerifying model files...\")\n",
    "        missing_files = []\n",
    "        for model in model_names:\n",
    "            model_file = Path(nc_dir) / f\"{model}.nc\"\n",
    "            if not model_file.exists():\n",
    "                missing_files.append(f\"{model}.nc\")\n",
    "\n",
    "        if missing_files:\n",
    "            print(f\"ERROR: Missing model files: {missing_files}\")\n",
    "            return\n",
    "        print(\"All model files found!\")\n",
    "\n",
    "        # Read shapefiles\n",
    "        print(\"\\nReading shapefiles...\")\n",
    "        basins_gdf = gpd.read_file(basin_shapefile)\n",
    "        jordan_gdf = gpd.read_file(gov_shapefile)\n",
    "\n",
    "        # Verify coordinate systems\n",
    "        print(\"\\nCoordinate Reference Systems:\")\n",
    "        print(f\"Basins: {basins_gdf.crs}\")\n",
    "        print(f\"Jordan: {jordan_gdf.crs}\")\n",
    "\n",
    "        # Create basin mask using sample NetCDF for coordinates\n",
    "        print(\"\\nCreating basin mask...\")\n",
    "        sample_ds = xr.open_dataset(Path(nc_dir) / f\"{model_names[0]}.nc\")\n",
    "        basin_mask = create_basin_mask(basins_gdf, sample_ds.lat.values, sample_ds.lon.values, jordan_gdf)\n",
    "        sample_ds.close()\n",
    "\n",
    "        # Count valid grid points\n",
    "        valid_points = np.sum(basin_mask != None)\n",
    "        total_points = basin_mask.size\n",
    "        print(\n",
    "            f\"Valid grid points (within Jordan basins): {valid_points}/{total_points} ({valid_points / total_points * 100:.1f}%)\")\n",
    "\n",
    "        # Define time periods\n",
    "        periods = [\n",
    "            ('1961_1994', '1961-01-01', '1994-12-31'),\n",
    "            ('1995_2014', '1995-01-01', '2014-12-31'),\n",
    "            ('2015_2020', '2015-01-01', '2020-12-31'),\n",
    "            ('2021_2040', '2021-01-01', '2040-12-31'),\n",
    "            ('2041_2060', '2041-01-01', '2060-12-31'),\n",
    "            ('2061_2070', '2061-01-01', '2070-12-31')\n",
    "        ]\n",
    "\n",
    "        # Create ensemble datasets for each period\n",
    "        successful_periods = []\n",
    "        for period_name, start_date, end_date in periods:\n",
    "            output_path = Path(output_dir) / f\"ensemble_precipitation_6models_{period_name}.nc\"\n",
    "            print(f\"\\n{'=' * 50}\")\n",
    "            print(f\"Processing period {period_name}...\")\n",
    "\n",
    "            try:\n",
    "                ds = create_6_model_ensemble_for_period(\n",
    "                    nc_dir, basin_mask, model_names,\n",
    "                    start_date, end_date, output_path\n",
    "                )\n",
    "\n",
    "                # Print basic statistics for verification\n",
    "                print(f\"Dataset summary for {period_name}:\")\n",
    "                print(f\"  Time steps: {len(ds.time)}\")\n",
    "                print(f\"  Grid points with data: {(ds.model_count > 0).sum().values.item()}\")\n",
    "                print(f\"  Average models per point: {ds.model_count[ds.model_count > 0].mean().values:.1f}\")\n",
    "                print(f\"  Non-zero precipitation points: {(ds.prAdjust != 0).sum().values.item()}\")\n",
    "                print(f\"Successfully saved to {output_path}\")\n",
    "\n",
    "                successful_periods.append(period_name)\n",
    "                ds.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing period {period_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Create summary report\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(\"Creating summary report...\")\n",
    "        create_ensemble_summary_report(output_dir, model_names)\n",
    "\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(\"PROCESSING COMPLETED!\")\n",
    "        print(f\"Successfully processed {len(successful_periods)}/{len(periods)} periods\")\n",
    "        print(f\"Output files and summary report can be found in: {output_dir}\")\n",
    "        print(f\"Models used: {', '.join(model_names)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
